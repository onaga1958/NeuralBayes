{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8KxW17ljMVbF"
   },
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tsBLwpKdMVbH"
   },
   "source": [
    "# Variational Dropout Sparsifies Deep Neural Networks\n",
    "\n",
    "The Variational Dropout (https://arxiv.org/abs/1506.02557) provides a Bayesian interpretation of conventional dropout procedure. Later it was shown that Variational Dropout can be used for model sparsification (Sparse VD), the effect can be achieved by via optimization of variational lower bound wrt individual dropout rates for every weight of the model (https://arxiv.org/abs/1701.05369).\n",
    "\n",
    "#### Sparse VD\n",
    "\n",
    "Sparse VD model optimizes VLB $\\mathcal{L}(\\phi)$ with respect to parameters $\\phi$ of a variational approximation $q_\\phi(w)$:\n",
    "\n",
    "$$\\mathcal{L}(\\phi) =  L_\\mathcal{D}(\\phi) - D_{KL}(q_\\phi(w)\\,\\|\\,p(w)) \\to\\max_{\\phi\\in\\Phi}$$\n",
    "$$L_\\mathcal{D}(\\phi) = \\sum_{n=1}^N \\mathrm{E}_{q_\\phi(w)}[\\log p(y_n\\,|\\,x_n, w)],$$\n",
    "\n",
    "where $p(w)$ is the log-uniform prior distibution, the variational approxinmation $q_\\phi(w)$ is a fullly factorized gaussian, the likelihood $p(y\\,|\\,x, w)$ is defined by a neuralnework with parametrs $w$. The optimization is performed by stohasic optimization methods e.g., Adam, etc.\n",
    "\n",
    "For more convenience computing, the KL divergence is approximated as follows:\n",
    "\\begin{equation}\n",
    "\\begin{gathered}\n",
    "    -D_{KL}(q(w_{ij}\\,|\\,\\theta_{ij}, \\alpha_{ij})\\,\\|\\,p(w_{ij})) \\approx\n",
    "    \\\\\n",
    "    \\approx k_1\\sigma(k_2 + k_3\\log \\alpha_{ij})) - 0.5\\log(1+\\alpha_{ij}^{-1}) + \\mathrm{C}\n",
    "    \\label{eq:KL}\\\\\n",
    "    k_1=0.63576~~~~~k_2=1.87320~~~~~k_3=1.48695\n",
    "\\end{gathered}\n",
    "\\end{equation}\n",
    "\n",
    "**Note:** In the paper two parametrizations of q are used. The fist one is $\\phi_i=\\{\\mu_{i}, \\sigma_i\\}$ that means $w_{ij} \\sim N(w_{ij} | \\mu_{ij}, \\sigma^2_{ij})$ and the second one is $\\phi_{ij}=\\{\\mu_{ij}, \\alpha_{ij}\\}$ that means $w_{ij} \\sim N(w_{ij} | \\mu_{ij}, \\alpha_{ij}\\mu^2_{ij})$. This two parametrization are connected as $\\sigma^2_{ij} = \\alpha_{ij}\\mu^2_{ij}$. Do not be confused.\n",
    "\n",
    "\n",
    "# In this assignment:\n",
    "1. Implementation of fully-connected Sparse VD layer\n",
    "2. Training Lenet-300-100 on MNIST dataset\n",
    "3. Optional Research Assignment\n",
    "\n",
    "Additional information:\n",
    "- If you have a problem with importing logger, download logger.py and file to the same folder and run a notebook from it\n",
    "- You will need the following python packages: pytorch, numpy, sklearn, pylab (matplotlib), tabulate\n",
    "- If you have an urgent question or find a typo or a mistake, send it to ars.ashuha@gmail.com. The title should include \"BDL Assignment 3, 2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsimFopvMVbI"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from logger import Logger\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dztg-5hDMVbM"
   },
   "source": [
    "## Implementation of  Sparse VD layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rskYuZWkMVbN"
   },
   "outputs": [],
   "source": [
    "class LinearSVDO(nn.Module):\n",
    "    def __init__(self, in_features, out_features, threshold, bias=True):\n",
    "        super(LinearSVDO, self).__init__()\n",
    "        \"\"\"\n",
    "            in_features: int, a number of input features\n",
    "            out_features: int, a number of neurons\n",
    "            threshold: float, a threshold for clipping weights\n",
    "        \"\"\"\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.mu = Parameter(torch.zeros(out_features, in_features)) # torch.nn.parameter.Parameter of size out_features x in_features\n",
    "        self.log_sigma = Parameter(torch.zeros(out_features, in_features)) # torch.nn.parameter.Parameter of size out_features x in_features\n",
    "        self.bias = Parameter(torch.zeros(1, out_features)) # torch.nn.parameter.Parameter of size 1 x out_features\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.bias.data.zero_()\n",
    "        self.mu.data.normal_(0, 0.02)\n",
    "        self.log_sigma.data.fill_(-5)        \n",
    "        \n",
    "    def forward(self, x):      \n",
    "        # x is a torch.Tensor of shape (?number_of_objects, in_features)\n",
    "        # log_alpha is a torch.Tensor of shape (out_features, in_features)\n",
    "        # Compute using self.log_sigma and self.mu\n",
    "        self.log_alpha = 2 * (self.log_sigma - torch.log(self.mu))\n",
    "        # clipping for a numerical stability\n",
    "        self.log_alpha = torch.clamp(self.log_alpha, -10, 10)   \n",
    "        \n",
    "        if self.training:\n",
    "            # lrt_mean is a torch.Tensor of shape (x.shape[0], out_features)\n",
    "            # compute mean activation using LRT\n",
    "            lrt_mean = torch.matmul(x, torch.transpose(self.mu, 0, 1)) + self.bias \n",
    "            # lrt_std is a torch.Tensor of shape (x.shape[0], out_features)\n",
    "            # compute std of activations unsig lrt, \n",
    "            # do not forget use torch.sqrt(x + 1e-8) instead of torch.sqrt(x)\n",
    "            sigma = torch.transpose(torch.exp(2 * self.log_sigma), 0, 1)\n",
    "            lrt_std = torch.sqrt(torch.matmul(x ** 2, sigma) + 1e-8)\n",
    "\n",
    "            # eps is a torch.Tensor of shape (x.shape[0], out_features)\n",
    "            eps = torch.randn(x.shape[0], self.out_features)\n",
    "            # sample of noise for reparametrization\n",
    "            return eps * lrt_std + lrt_mean  # sample of activation\n",
    "        weights = self.mu * (self.log_alpha <= self.threshold).type(torch.float)\n",
    "        out = torch.matmul(x, torch.transpose(weights, 0, 1)) + self.bias  # compute the output of the layer\n",
    "        # use weighs W = Eq = self.mu\n",
    "        # clip all weight with log_alpha > threshold\n",
    "        return out\n",
    "        \n",
    "    def kl_reg(self):\n",
    "        k1, k2, k3 = torch.Tensor([0.63576]), torch.Tensor([1.8732]), torch.Tensor([1.48695])\n",
    "        # kl is a scalar torch.Tensor \n",
    "        kl = -k1 * torch.sigmoid(k2 + k3 * self.log_alpha) - 0.5 * torch.nn.functional.logsigmoid(self.log_alpha)\n",
    "        # eval KL using the approximation\n",
    "        return torch.mean(kl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-uo1Ur-MMVbQ"
   },
   "source": [
    "## Define LeNet-300-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9bZmsqU3MVbQ"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, threshold):\n",
    "        super().__init__()\n",
    "        self.fc1 = LinearSVDO(28 * 28, 300, threshold)\n",
    "        self.fc2 = LinearSVDO(300, 100, threshold)\n",
    "        self.fc3 = LinearSVDO(100, 10, threshold)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-0I5jarMVbT"
   },
   "source": [
    "## Function for loading MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SjgqcgLMVbT"
   },
   "outputs": [],
   "source": [
    "def get_mnist(batch_size):\n",
    "    trsnform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Jb8iFigMVbX"
   },
   "source": [
    "## Create SGVLB loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-oQVbqXMVbY"
   },
   "outputs": [],
   "source": [
    "class SGVLB(nn.Module):\n",
    "    def __init__(self, net, train_size):\n",
    "        super(SGVLB, self).__init__()\n",
    "        self.train_size = train_size # int, the len of dataset\n",
    "        self.net = net # nn.Module\n",
    "        \n",
    "    def forward(self, input, target, kl_weight=1.0):\n",
    "        assert not target.requires_grad\n",
    "        kl = 0.0\n",
    "        for module in self.net.children():\n",
    "            if hasattr(module, 'kl_reg'):\n",
    "                kl = kl + module.kl_reg()\n",
    "\n",
    "        sgvlb_loss = -torch.mean(input[:, target]) + kl  # a scalar torch.Tensor, SGVLB loss\n",
    "        return sgvlb_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xwIkQdlMVba"
   },
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S7t1Me8eMVbb"
   },
   "outputs": [],
   "source": [
    "model = Net(threshold=3)\n",
    "optimizer = torch.optim.Adam(model.parameters()) # optimizer\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 100)\n",
    "# decrease learning rate by torch.optim.lr_scheduler\n",
    "\n",
    "fmt = {'tr_los': '3.1e', 'te_loss': '3.1e', 'sp_0': '.3f', 'sp_1': '.3f', 'lr': '3.1e', 'kl': '.2f'}\n",
    "logger = Logger('sparse_vd', fmt=fmt)\n",
    "\n",
    "train_loader, test_loader = get_mnist(batch_size=100)\n",
    "sgvlb = SGVLB(model, len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06mxgwp2MVbe"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWinHfniMVbf",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    kl       lr    tr_los    tr_acc    te_loss    te_acc    sp_0    sp_1    sp_2\n",
      "-------  ----  -------  --------  --------  ---------  --------  ------  ------  ------\n",
      "      1  0.04  1.0e-03       nan       9.9        nan       9.8   0.000   0.000     0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onaga/Library/Python/3.6/lib/python/site-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2  0.06  1.0e-03       nan       9.9        nan       9.8   0.000   0.000     0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-18c20c026ce0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgvlb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-4336968fd59d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target, kl_weight)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kl_reg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msgvlb_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mkl\u001b[0m  \u001b[0;31m# a scalar torch.Tensor, SGVLB loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-a0debf23cf62>\u001b[0m in \u001b[0;36mkl_reg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.63576\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.8732\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.48695\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# kl is a scalar torch.Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mk1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_alpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_alpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;31m# eval KL using the approximation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kl_weight = 0.02\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    scheduler.step()\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0 \n",
    "    kl_weight = min(kl_weight + 0.02, 1)\n",
    "    logger.add_scalar(epoch, 'kl', kl_weight)\n",
    "    logger.add_scalar(epoch, 'lr', scheduler.get_lr()[0])\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        pred = output.data.max(1)[1] \n",
    "        loss = sgvlb(output, target, kl_weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += float(loss) \n",
    "        train_acc += np.sum(pred.numpy() == target.data.numpy())\n",
    "\n",
    "    logger.add_scalar(epoch, 'tr_los', train_loss / len(train_loader.dataset))\n",
    "    logger.add_scalar(epoch, 'tr_acc', train_acc / len(train_loader.dataset) * 100)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        output = model(data)\n",
    "        test_loss += float(sgvlb(output, target, kl_weight))\n",
    "        pred = output.data.max(1)[1] \n",
    "        test_acc += np.sum(pred.numpy() == target.data.numpy())\n",
    "        \n",
    "    logger.add_scalar(epoch, 'te_loss', test_loss / len(test_loader.dataset))\n",
    "    logger.add_scalar(epoch, 'te_acc', test_acc / len(test_loader.dataset) * 100)\n",
    "    \n",
    "    for i, c in enumerate(model.children()):\n",
    "        if hasattr(c, 'kl_reg'):\n",
    "            logger.add_scalar(epoch, 'sp_%s' % i, (c.log_alpha.data.numpy() > model.threshold).mean())\n",
    "            \n",
    "    logger.iter_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9BYVKxAMVbj"
   },
   "outputs": [],
   "source": [
    "all_w, kep_w = 0, 0\n",
    "\n",
    "for c in model.children():\n",
    "    kep_w += (c.log_alpha.data.numpy() < model.threshold).sum()\n",
    "    all_w += c.log_alpha.data.numpy().size\n",
    "\n",
    "# compression_ratio shold be > 30\n",
    "compression_ratio = all_w / kep_w\n",
    "print('compression_ratio =', compression_ratio)\n",
    "assert compression_ratio > 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGpOF9mzMVbl"
   },
   "source": [
    "## Disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BjUuLQTQMVbl"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix, csc_matrix, coo_matrix, dok_matrix\n",
    "\n",
    "row, col, data = [], [], []\n",
    "M = list(model.children())[0].W.data.numpy()\n",
    "LA = list(model.children())[0].log_alpha.data.numpy()\n",
    "\n",
    "for i in range(300):\n",
    "    for j in range(28 * 28):\n",
    "        if LA[i, j] < 3:\n",
    "            row += [i]\n",
    "            col += [j]\n",
    "            data += [M[i, j]]\n",
    "\n",
    "Mcsr = csc_matrix((data, (row, col)), shape=(300, 28 * 28))\n",
    "Mcsc = csc_matrix((data, (row, col)), shape=(300, 28 * 28))\n",
    "Mcoo = coo_matrix((data, (row, col)), shape=(300, 28 * 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZzWIiMr3MVbn"
   },
   "outputs": [],
   "source": [
    "np.savez_compressed('M_w', M)\n",
    "scipy.sparse.save_npz('Mcsr_w', Mcsr)\n",
    "scipy.sparse.save_npz('Mcsc_w', Mcsc)\n",
    "scipy.sparse.save_npz('Mcoo_w', Mcoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HojA6M4_MVbp"
   },
   "outputs": [],
   "source": [
    "!ls -lah | grep .npz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXtYpKL7MVbs"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MahGlBp_MVbt"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 16, 3\n",
    "rcParams['figure.dpi'] = 300\n",
    "\n",
    "\n",
    "log_alpha = (model.fc1.log_alpha.detach().numpy() < 3).astype(np.float)\n",
    "W = model.fc1.W.detach().numpy()\n",
    "\n",
    "plt.imshow(log_alpha * W, cmap='hot', interpolation=None)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdKgBhs-MVbv"
   },
   "outputs": [],
   "source": [
    "s = 0\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 8, 5\n",
    "\n",
    "z = np.zeros((28 * 15, 28 * 15))\n",
    "\n",
    "for i in range(15):\n",
    "    for j in range(15):\n",
    "        s += 1\n",
    "        z[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] =  np.abs((log_alpha * W)[s].reshape(28, 28))\n",
    "        \n",
    "plt.imshow(z, cmap='hot_r')\n",
    "plt.colorbar()\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aHJdsdqAMVby"
   },
   "source": [
    "# Optional Research Assignment (up to 2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cN8tSaevMVbz"
   },
   "source": [
    "1. Study the model: \n",
    "    - How sparsity and accuracy depend on maximum of KL-multiplier (kl_weight)?\n",
    "    - How quality depends on the initialization of log_sigma (log_sigma)?\n",
    "    - Study the KL approximation: what if we use the reparametrization trick to obtain an unbiased MC estimate of KL?\n",
    "2. Compression:\n",
    "    - What can we do to obtain better compression results with small quality degradation?\n",
    "    - Propose and eval several options.\n",
    "3. Study the Local reparametrization trick: \n",
    "    - Does it really accelerate convergence?\n",
    "    - Does variance of gradient decrease?\n",
    "    \n",
    "You can do one out of three parts. You need to provide evidence for results e.g., plots, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jNY6wg6MVb0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SparseVD-assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
